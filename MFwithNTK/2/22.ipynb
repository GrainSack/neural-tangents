{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXvB6JPaA6R+rzZCp/xv79",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrainSack/neural-tangents/blob/main/MFwithNTK/2/22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7Zk7Ni-yxq7",
        "outputId": "3ccfd29e-6874-4939-fc30-950a6c6f6d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/neural_collaborative_filtering-master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNpsFWYZy0Gn",
        "outputId": "5cd492a4-063f-434e-8bea-9537d1610e5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/neural_collaborative_filtering-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width: 75% !important; }</style>\"))\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "WtoE56JEzA4C",
        "outputId": "125e7d7e-c703-4e21-93bd-0b8e45ebdd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width: 75% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "bwPihRr_2e6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Neural_CF-master/lastfm-dataset-360K'\n",
        "df = pd.read_csv(file_path + '/usersha1-artmbid-artname-plays.tsv', delimiter='\\t', header=None)\n",
        "\n",
        "df = df.drop(df.columns[2], axis=1)\n",
        "df.columns = ['user', 'item', 'plays']\n",
        "df = df.dropna()\n",
        "df = df.loc[df.plays != 0]"
      ],
      "metadata": {
        "id": "ng9afmaszyQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYm94xyp2D2e",
        "outputId": "9c70d20b-9852-446e-9acd-7f79ddb8c23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17309518, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "F6BGwcIb_DbN",
        "outputId": "5f59bc6a-86b8-4792-855c-89881d841c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       user  \\\n",
              "0  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
              "1  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
              "2  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
              "3  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
              "4  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
              "\n",
              "                                   item  plays  \n",
              "0  3bd73256-3905-4f3a-97e2-8b341527f805   2137  \n",
              "1  f2fb0ff0-5679-42ec-a55c-15109ce6e320   1099  \n",
              "2  b3ae82c2-e60b-4551-a76d-6620f1b456aa    897  \n",
              "3  3d6bbeb7-f90e-4d10-b440-e153c0d10b53    717  \n",
              "4  bbd2ffd7-17f4-4506-8572-c1ea58c3f9a8    706  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60f03850-4f9f-4e41-b864-4db24cc9e46c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>item</th>\n",
              "      <th>plays</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
              "      <td>3bd73256-3905-4f3a-97e2-8b341527f805</td>\n",
              "      <td>2137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
              "      <td>f2fb0ff0-5679-42ec-a55c-15109ce6e320</td>\n",
              "      <td>1099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
              "      <td>b3ae82c2-e60b-4551-a76d-6620f1b456aa</td>\n",
              "      <td>897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
              "      <td>3d6bbeb7-f90e-4d10-b440-e153c0d10b53</td>\n",
              "      <td>717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
              "      <td>bbd2ffd7-17f4-4506-8572-c1ea58c3f9a8</td>\n",
              "      <td>706</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60f03850-4f9f-4e41-b864-4db24cc9e46c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60f03850-4f9f-4e41-b864-4db24cc9e46c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60f03850-4f9f-4e41-b864-4db24cc9e46c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('user 수:', len(np.unique(list(df['user'])))) \n",
        "print('artist 수:', len(np.unique(list(df['item']))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9gWIMqA_Fp0",
        "outputId": "3315573c-178b-45ba-9301-85604011e643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user 수: 358858\n",
            "artist 수: 160112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_analy_dataset(df):\n",
        "    \"\"\"\n",
        "    데이터 로드 함수 \n",
        "    \n",
        "    uids: train user\n",
        "    iids: train item\n",
        "    users: 전체 user          \n",
        "    items: 전체 item\n",
        "    df_train: train data\n",
        "    df_test: test data\n",
        "    \"\"\"\n",
        "    # user 10000명 샘플링 \n",
        "    unique_user_lst = list(np.unique(df['user'])) #358857명 \n",
        "    sample_user_idx = np.random.choice(len(unique_user_lst), 10000, replace=False)\n",
        "    sample_user_lst = [unique_user_lst[idx] for idx in sample_user_idx]\n",
        "    \n",
        "    df = df[df['user'].isin(sample_user_lst)]\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # 1명 이상의 artist데이터가 있는 user만 사용 \n",
        "    df_count = df.groupby(['user']).count()\n",
        "    df['count'] = df.groupby('user')['user'].transform('count')\n",
        "    df = df[df['count'] > 1]\n",
        "\n",
        "    # user, item 아이디 부여 \n",
        "    df['user_id'] = df['user'].astype(\"category\").cat.codes\n",
        "    df['item_id'] = df['item'].astype(\"category\").cat.codes\n",
        "\n",
        "    # lookup 테이블 생성 \n",
        "    item_lookup = df[['item_id', 'item']].drop_duplicates()\n",
        "    item_lookup['item_id'] = item_lookup.item_id.astype(str)\n",
        "\n",
        "    # train, test 데이터 생성 \n",
        "    df = df[['user_id', 'item_id', 'plays']] \n",
        "    df_train, df_test = train_test_split(df)\n",
        "\n",
        "    # 전체 user, item 리스트 생성 \n",
        "    users = list(np.sort(df.user_id.unique())) \n",
        "    items = list(np.sort(df.item_id.unique())) \n",
        "\n",
        "    # train user, item 리스트 생성 \n",
        "    rows = df_train['user_id'].astype(int)   \n",
        "    cols = df_train['item_id'].astype(int)\n",
        "    values = list(df_train.plays) \n",
        "    \n",
        "    uids = np.array(rows.tolist())\n",
        "    iids = np.array(cols.tolist())\n",
        "\n",
        "    # 각 user마다 negative item 생성 \n",
        "    df_neg = get_negatives(uids, iids, items, df_test)\n",
        "\n",
        "    return uids, iids, df_train, df_test, df_neg, users, items, item_lookup\n",
        "\n",
        "def get_negatives(uids, iids, items, df_test):\n",
        "    \"\"\"\n",
        "    negative item 리스트 생성함수\n",
        "    \"\"\"\n",
        "    negativeList = []\n",
        "    test_u = df_test['user_id'].values.tolist() \n",
        "    test_i = df_test['item_id'].values.tolist() \n",
        " \n",
        "    test_ratings = list(zip(test_u, test_i)) # test (user, item)세트 \n",
        "    zipped = set(zip(uids, iids))            # train (user, item)세트\n",
        "\n",
        "    for (u, i) in test_ratings:\n",
        "        \n",
        "        negatives = []\n",
        "        negatives.append((u, i))\n",
        "        for t in range(100):\n",
        "            j = np.random.randint(len(items))     # neg_item j 1개 샘플링 \n",
        "            while (u, j) in zipped:               # j가 train에 있으면 다시뽑고, 없으면 선택 \n",
        "                j = np.random.randint(len(items)) \n",
        "            negatives.append(j)\n",
        "        negativeList.append(negatives) # [(0,pos), neg, neg, ...]\n",
        "\n",
        "    df_neg = pd.DataFrame(negativeList)\n",
        "\n",
        "    return df_neg\n",
        "\n",
        "def mask_first(x):\n",
        "\n",
        "    result = np.ones_like(x) \n",
        "    result[0] = 0  # [0,1,1,....]\n",
        "    \n",
        "    return result\n",
        "\n",
        "def train_test_split(df):\n",
        "    \"\"\"\n",
        "    train, test 나누는 함수\n",
        "    \"\"\"\n",
        "    df_test = df.copy(deep=True)\n",
        "    df_train = df.copy(deep=True)\n",
        "    \n",
        "    # df_test\n",
        "    # user_id와 holdout_item_id(user가 플레이한 아이템 중 1개)뽑기 \n",
        "    df_test = df_test.groupby(['user_id']).first() \n",
        "    df_test['user_id'] = df_test.index\n",
        "    df_test = df_test[['user_id', 'item_id', 'plays']]\n",
        "    df_test = df_test.reset_index(drop=True)\n",
        "    \n",
        "    # df_train \n",
        "    # user_id 리스트에 make_first()적용 \n",
        "    mask = df.groupby(['user_id'])['user_id'].transform(mask_first).astype(bool)\n",
        "    df_train = df.loc[mask]  \n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "def get_train_instances(uids, iids, num_neg, num_items):\n",
        "    \"\"\"\n",
        "    모델에 사용할 train 데이터 생성 함수 \n",
        "    \"\"\"\n",
        "    user_input, item_input, labels = [],[],[]\n",
        "    zipped = set(zip(uids, iids)) # train (user, item) 세트\n",
        "\n",
        "    for (u, i) in zip(uids, iids):\n",
        "        \n",
        "        # pos item 추가 \n",
        "        user_input.append(u) #[u]\n",
        "        item_input.append(i) #[pos_i]\n",
        "        labels.append(1)     #[1]\n",
        "\n",
        "        # neg item 추가 \n",
        "        for t in range(num_neg):\n",
        "            \n",
        "            j = np.random.randint(num_items)     # neg_item j num_neg(4)개 샘플링\n",
        "            while (u, j) in zipped:              # u가 j를 이미 선택했다면 \n",
        "                j = np.random.randint(num_items) # 다시 샘플링 \n",
        "                \n",
        "            user_input.append(u) # [u1, u1,  u1,  ...]\n",
        "            item_input.append(j) # [pos_i, neg_j1, neg_j2, ...]\n",
        "            labels.append(0)     # [1, 0,  0,  ...]\n",
        "\n",
        "    return user_input, item_input, labels"
      ],
      "metadata": {
        "id": "IdiIxmAf_Hd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uids, iids, df_train, df_test, df_neg, users, items, item_lookup = prepare_analy_dataset(df)"
      ],
      "metadata": {
        "id": "wC-88Y88_KIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "iXuD6r_z_LsN",
        "outputId": "fb6ae19b-5017-45b6-acda-10a13ed9589f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    user_id  item_id  plays\n",
              "1         0    32117   1540\n",
              "2         0     7898   1403\n",
              "3         0    15735   1308\n",
              "4         0     4388   1214\n",
              "5         0     5478   1197\n",
              "6         0     3822    945\n",
              "7         0    44460    895\n",
              "8         0    47358    887\n",
              "9         0     3912    833\n",
              "10        0     7939    721"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0bb1ad3-2939-467b-a217-9679278328ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>plays</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>32117</td>\n",
              "      <td>1540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>7898</td>\n",
              "      <td>1403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>15735</td>\n",
              "      <td>1308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4388</td>\n",
              "      <td>1214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>5478</td>\n",
              "      <td>1197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>3822</td>\n",
              "      <td>945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>44460</td>\n",
              "      <td>895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>47358</td>\n",
              "      <td>887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>3912</td>\n",
              "      <td>833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>7939</td>\n",
              "      <td>721</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0bb1ad3-2939-467b-a217-9679278328ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a0bb1ad3-2939-467b-a217-9679278328ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a0bb1ad3-2939-467b-a217-9679278328ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuzF7NJb_M8Q",
        "outputId": "e661f68a-f7f1-4a36-ba59-6dcf2f79a1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(472097, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input, item_input, labels = get_train_instances(uids, iids, num_neg=4, num_items=len(items))"
      ],
      "metadata": {
        "id": "EgmeZ3s6_Ohq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('df_train의 첫번째 행: (user_id, item_id)=', (uids[0], iids[0])) \n",
        "print('df_train의 두번째 행: (user_id, item_id)=', (uids[1], iids[1])) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU9Zcv24_P4O",
        "outputId": "9fa2d263-c64a-4ede-a4fc-b332ddfcf576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_train의 첫번째 행: (user_id, item_id)= (0, 32117)\n",
            "df_train의 두번째 행: (user_id, item_id)= (0, 7898)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (user_id, item_id, label) in enumerate(zip(user_input[0:10], item_input[0:10], labels[0:10])):\n",
        "    if i==0 or i==5:\n",
        "        print('(user_id, postive_item_id, label):', (user_id, item_id, label))\n",
        "    else:\n",
        "        print('(user_id, negative_item_id, label):', (user_id, item_id, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf_iUN2D_RYQ",
        "outputId": "1594dbbb-baab-43fd-b377-0ec6e9d095f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(user_id, postive_item_id, label): (0, 32117, 1)\n",
            "(user_id, negative_item_id, label): (0, 16826, 0)\n",
            "(user_id, negative_item_id, label): (0, 38790, 0)\n",
            "(user_id, negative_item_id, label): (0, 40056, 0)\n",
            "(user_id, negative_item_id, label): (0, 36382, 0)\n",
            "(user_id, postive_item_id, label): (0, 7898, 1)\n",
            "(user_id, negative_item_id, label): (0, 4910, 0)\n",
            "(user_id, negative_item_id, label): (0, 28710, 0)\n",
            "(user_id, negative_item_id, label): (0, 30522, 0)\n",
            "(user_id, negative_item_id, label): (0, 36932, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "WfZqfyYg_SzM",
        "outputId": "f45469af-3c6a-4630-bec5-555a16f511b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   user_id  item_id  plays\n",
              "0        0    40780   2618\n",
              "1        1    15453    744\n",
              "2        2    37624    157\n",
              "3        3    33130    802\n",
              "4        4    25257    341\n",
              "5        5    21499    835\n",
              "6        6     1227   1046\n",
              "7        7    11773   4418\n",
              "8        8     2215    168\n",
              "9        9    30763   1776"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9b0f3f43-600d-4f61-8ecd-273a1828b59f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>plays</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>40780</td>\n",
              "      <td>2618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>15453</td>\n",
              "      <td>744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>37624</td>\n",
              "      <td>157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>33130</td>\n",
              "      <td>802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>25257</td>\n",
              "      <td>341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>21499</td>\n",
              "      <td>835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>1227</td>\n",
              "      <td>1046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>11773</td>\n",
              "      <td>4418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>2215</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>30763</td>\n",
              "      <td>1776</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b0f3f43-600d-4f61-8ecd-273a1828b59f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9b0f3f43-600d-4f61-8ecd-273a1828b59f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9b0f3f43-600d-4f61-8ecd-273a1828b59f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j61x3cNs_UP0",
        "outputId": "6a2b618b-cddd-48d7-ea04-e7b7cd7b9ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9997, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_neg.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "tQzFKBHo_VXn",
        "outputId": "c80ae288-b566-4f63-c485-59ba654ddd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0      1      2      3      4      5      6      7      8      9    \\\n",
              "0  (0, 40780)  18447  14159  44985  43226  12988  38499  40761  18261  14179   \n",
              "1  (1, 15453)  37389  38644   6809  10012  45516  12896   3300   1927  30358   \n",
              "2  (2, 37624)  32649   5050  34405  48404  43756  19581   1985  36912  28457   \n",
              "3  (3, 33130)   7706  24412  18241  38745  43345  38608  43129  44668   2578   \n",
              "4  (4, 25257)  24924   9387  36992   5516  30135  10238  40738  44509   5677   \n",
              "\n",
              "   ...    91     92     93     94     95     96     97     98     99     100  \n",
              "0  ...  24176  25296    421  37182  47583  11273  17766  10523   9190  41012  \n",
              "1  ...   6337  38598   1264  24409  47353  44961   1350    136  36001  38596  \n",
              "2  ...  39536   7744  13616  11921   1140  24986   8178  19577  37338  18657  \n",
              "3  ...  14984  36433  10382  13264  42512  37398  28594  16568  23680   1532  \n",
              "4  ...   4496  26082   2747  15780  23746   8108  31261  20875    368  47955  \n",
              "\n",
              "[5 rows x 101 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc116bb0-6952-4171-be25-9d61cddc35cd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(0, 40780)</td>\n",
              "      <td>18447</td>\n",
              "      <td>14159</td>\n",
              "      <td>44985</td>\n",
              "      <td>43226</td>\n",
              "      <td>12988</td>\n",
              "      <td>38499</td>\n",
              "      <td>40761</td>\n",
              "      <td>18261</td>\n",
              "      <td>14179</td>\n",
              "      <td>...</td>\n",
              "      <td>24176</td>\n",
              "      <td>25296</td>\n",
              "      <td>421</td>\n",
              "      <td>37182</td>\n",
              "      <td>47583</td>\n",
              "      <td>11273</td>\n",
              "      <td>17766</td>\n",
              "      <td>10523</td>\n",
              "      <td>9190</td>\n",
              "      <td>41012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(1, 15453)</td>\n",
              "      <td>37389</td>\n",
              "      <td>38644</td>\n",
              "      <td>6809</td>\n",
              "      <td>10012</td>\n",
              "      <td>45516</td>\n",
              "      <td>12896</td>\n",
              "      <td>3300</td>\n",
              "      <td>1927</td>\n",
              "      <td>30358</td>\n",
              "      <td>...</td>\n",
              "      <td>6337</td>\n",
              "      <td>38598</td>\n",
              "      <td>1264</td>\n",
              "      <td>24409</td>\n",
              "      <td>47353</td>\n",
              "      <td>44961</td>\n",
              "      <td>1350</td>\n",
              "      <td>136</td>\n",
              "      <td>36001</td>\n",
              "      <td>38596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(2, 37624)</td>\n",
              "      <td>32649</td>\n",
              "      <td>5050</td>\n",
              "      <td>34405</td>\n",
              "      <td>48404</td>\n",
              "      <td>43756</td>\n",
              "      <td>19581</td>\n",
              "      <td>1985</td>\n",
              "      <td>36912</td>\n",
              "      <td>28457</td>\n",
              "      <td>...</td>\n",
              "      <td>39536</td>\n",
              "      <td>7744</td>\n",
              "      <td>13616</td>\n",
              "      <td>11921</td>\n",
              "      <td>1140</td>\n",
              "      <td>24986</td>\n",
              "      <td>8178</td>\n",
              "      <td>19577</td>\n",
              "      <td>37338</td>\n",
              "      <td>18657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(3, 33130)</td>\n",
              "      <td>7706</td>\n",
              "      <td>24412</td>\n",
              "      <td>18241</td>\n",
              "      <td>38745</td>\n",
              "      <td>43345</td>\n",
              "      <td>38608</td>\n",
              "      <td>43129</td>\n",
              "      <td>44668</td>\n",
              "      <td>2578</td>\n",
              "      <td>...</td>\n",
              "      <td>14984</td>\n",
              "      <td>36433</td>\n",
              "      <td>10382</td>\n",
              "      <td>13264</td>\n",
              "      <td>42512</td>\n",
              "      <td>37398</td>\n",
              "      <td>28594</td>\n",
              "      <td>16568</td>\n",
              "      <td>23680</td>\n",
              "      <td>1532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(4, 25257)</td>\n",
              "      <td>24924</td>\n",
              "      <td>9387</td>\n",
              "      <td>36992</td>\n",
              "      <td>5516</td>\n",
              "      <td>30135</td>\n",
              "      <td>10238</td>\n",
              "      <td>40738</td>\n",
              "      <td>44509</td>\n",
              "      <td>5677</td>\n",
              "      <td>...</td>\n",
              "      <td>4496</td>\n",
              "      <td>26082</td>\n",
              "      <td>2747</td>\n",
              "      <td>15780</td>\n",
              "      <td>23746</td>\n",
              "      <td>8108</td>\n",
              "      <td>31261</td>\n",
              "      <td>20875</td>\n",
              "      <td>368</td>\n",
              "      <td>47955</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 101 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc116bb0-6952-4171-be25-9d61cddc35cd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dc116bb0-6952-4171-be25-9d61cddc35cd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dc116bb0-6952-4171-be25-9d61cddc35cd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_neg.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjVA99Xg_WrY",
        "outputId": "db01ee97-2ddf-4908-fe0f-34ff55ffa02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9997, 101)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \\\n",
        "    libnvinfer-dev=6.0.1-1+cuda10.1 \\\n",
        "    libnvinfer-plugin6=6.0.1-1+cuda10.1"
      ],
      "metadata": {
        "id": "x9_DcZ47BDQC",
        "outputId": "e594e547-ae38-4a45-ce91-946e53cfa039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package libnvinfer6\n",
            "E: Version '6.0.1-1+cuda10.1' for 'libnvinfer-dev' was not found\n",
            "E: Unable to locate package libnvinfer-plugin6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export TF_CPP_MIN_LOG_LEVEL=\"2\""
      ],
      "metadata": {
        "id": "aES2HUtlK-QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhkKaX2hmpqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJQ8fEhFmw5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfZ-rJ-am4aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install neural-tangents\n",
        "# !pip install -r /content/drive/MyDrive/infinite_ac_cf_main/requirements.txt\n",
        "# !pip install sciPy"
      ],
      "metadata": {
        "id": "gdY9YhzoaCkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neural-tangents==0.5.0"
      ],
      "metadata": {
        "id": "kMgKheJtRc-h",
        "outputId": "0e2fac3a-3392-43dd-a14f-981b6ce464c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neural-tangents==0.5.0\n",
            "  Downloading neural_tangents-0.5.0-py2.py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from neural-tangents==0.5.0) (4.5.0)\n",
            "Collecting frozendict>=2.3\n",
            "  Downloading frozendict-2.3.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.2/111.2 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.3 in /usr/local/lib/python3.8/dist-packages (from neural-tangents==0.5.0) (0.3.25)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3->neural-tangents==0.5.0) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3->neural-tangents==0.5.0) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3->neural-tangents==0.5.0) (1.21.6)\n",
            "Installing collected packages: frozendict, neural-tangents\n",
            "Successfully installed frozendict-2.3.5 neural-tangents-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install jax==0.3.0\n",
        "!pip install jax jaxlib==0.3.0\n",
        "#!pip install jax tf2jax==0.3.0"
      ],
      "metadata": {
        "id": "j55F2Wlxad0p",
        "outputId": "2aacdeb6-b2d5-45b9-f340-24b674bcc905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.8/dist-packages (0.3.0)\n",
            "Collecting jaxlib==0.3.0\n",
            "  Downloading jaxlib-0.3.0-cp38-none-manylinux2010_x86_64.whl (65.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from jaxlib==0.3.0) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.8/dist-packages (from jaxlib==0.3.0) (1.21.6)\n",
            "Collecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from jaxlib==0.3.0) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from jax) (4.5.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax) (3.3.0)\n",
            "Installing collected packages: flatbuffers, jaxlib\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.1.21\n",
            "    Uninstalling flatbuffers-23.1.21:\n",
            "      Successfully uninstalled flatbuffers-23.1.21\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.25+cuda11.cudnn805\n",
            "    Uninstalling jaxlib-0.3.25+cuda11.cudnn805:\n",
            "      Successfully uninstalled jaxlib-0.3.25+cuda11.cudnn805\n",
            "Successfully installed flatbuffers-2.0.7 jaxlib-0.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jaxlib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip list | grep jax"
      ],
      "metadata": {
        "id": "m3rarfosaKbh",
        "outputId": "b40ba124-fc7f-437d-c091-651d3cd03256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jax                           0.3.0\n",
            "jaxlib                        0.3.0\n",
            "tf2jax                        0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/infinite_ac_cf_main"
      ],
      "metadata": {
        "id": "BfCuCxuVb-N-",
        "outputId": "e5b3df8b-f569-4c0f-edb2-e35e87035221",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/infinite_ac_cf_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "4wHTJkq1b-wU",
        "outputId": "b6ae0f74-0bdc-4e13-ffc1-8ecd1b1c9c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/infinite_ac_cf_main'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import neural_tangents"
      ],
      "metadata": {
        "id": "aDOJ8za7UqD3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from neural_tangents import stax"
      ],
      "metadata": {
        "id": "jzyytpuuTIx-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import"
      ],
      "metadata": {
        "id": "qe-MAKsDgmGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import numpy as np\n",
        "import functools\n",
        "import h5py, sys, os\n",
        "import copy\n",
        "import h5py\n",
        "import gc\n",
        "import pickle\n",
        "import jax.numpy as jnp\n",
        "import time\n",
        "import random\n",
        "import heapq\n",
        "from sklearn.utils import shuffle\n",
        "from jax import scipy as sp\n",
        "from jax import numpy as jnp\n",
        "#from neural_tangents import stax\n",
        "from collections import defaultdict\n",
        "from scipy.sparse import csr_matrix\n",
        "import os\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "os.environ[\"TF_FORCE_UNIFIED_MEMORY\"] = \"1\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\""
      ],
      "metadata": {
        "id": "vJrjZyYngkrV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "zfv189FjQFIK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "LwuPpMDucbAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_common_path(hyper_params):\n",
        "    ret = \"{}_users_{}_depth_{}_\".format(\n",
        "        hyper_params['dataset'], hyper_params['user_support'],\n",
        "        hyper_params['depth']\n",
        "    )\n",
        "    \n",
        "    if hyper_params['grid_search_lamda']: ret += \"grid_search_lamda_\"\n",
        "    else: ret += \"lamda_{}_\".format(hyper_params['lamda'])\n",
        "    \n",
        "    ret += \"seed_{}\".format(hyper_params['seed'])\n",
        "    return ret\n",
        "\n",
        "def get_item_count_map(data):\n",
        "    item_count = defaultdict(int)\n",
        "    for u, i, r in data.data['train']: item_count[i] += 1\n",
        "    return item_count\n",
        "\n",
        "def get_item_propensity(hyper_params, data, A = 0.55, B = 1.5):\n",
        "    item_freq_map = get_item_count_map(data)\n",
        "    item_freq = [ item_freq_map[i] for i in range(hyper_params['num_items']) ]\n",
        "    num_instances = hyper_params['num_interactions']\n",
        "\n",
        "    C = (np.log(num_instances)-1)*np.power(B+1, A)\n",
        "    wts = 1.0 + C*np.power(np.array(item_freq)+B, -A)\n",
        "    return np.ravel(wts)\n",
        "\n",
        "def file_write(log_file, s, dont_print=False):\n",
        "    if dont_print == False: print(s)\n",
        "    if log_file is None: return\n",
        "    f = open(log_file, 'a')\n",
        "    f.write(s+'\\n')\n",
        "    f.close()\n",
        "\n",
        "def log_end_epoch(hyper_params, metrics, step, time_elpased, metrics_on = '(TEST)', dont_print = False):\n",
        "    string2 = \"\"\n",
        "    for m in metrics: string2 += \" | \" + m + ' = ' + str(\"{:2.4f}\".format(metrics[m]))\n",
        "    string2 += ' ' + metrics_on\n",
        "\n",
        "    ss  = '| end of step {:4d} | time = {:5.2f}'.format(step, time_elpased)\n",
        "    ss += string2\n",
        "    file_write(hyper_params['log_file'], ss, dont_print = dont_print)"
      ],
      "metadata": {
        "id": "q1DaMnZAcaMt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "QSVndSQXbO-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#with open('test.pickle', 'rb') as f:\n",
        "   #test_pickle = pickle.load(f)\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, hyper_params):\n",
        "        self.data = load_raw_dataset(hyper_params['dataset'])\n",
        "        self.set_of_active_users = list(set(self.data['train'][:, 0].tolist()))            \n",
        "        self.hyper_params = self.update_hyper_params(hyper_params)\n",
        "\n",
        "    def update_hyper_params(self, hyper_params):\n",
        "        updated_params = copy.deepcopy(hyper_params)\n",
        "        \n",
        "        self.num_users, self.num_items = self.data['num_users'], self.data['num_items']\n",
        "        self.num_interactions = self.data['num_interactions']\n",
        "\n",
        "        # Update hyper-params to have some basic data stats\n",
        "        updated_params.update({\n",
        "            'num_users': self.num_users,\n",
        "            'num_items': self.num_items,\n",
        "            'num_interactions': self.num_interactions\n",
        "        })\n",
        "\n",
        "        return updated_params\n",
        "\n",
        "    def sample_users(self, num_to_sample):\n",
        "        if num_to_sample == -1: \n",
        "            ret = self.data['train_matrix']\n",
        "        else: \n",
        "            sampled_users = np.random.choice(self.set_of_active_users, num_to_sample, replace=False)\n",
        "            sampled_interactions = self.data['train'][np.in1d(self.data['train'][:, 0], sampled_users)]\n",
        "            ret = csr_matrix(\n",
        "                ( np.ones(sampled_interactions.shape[0]), (sampled_interactions[:, 0], sampled_interactions[:, 1]) ),\n",
        "                shape = (self.num_users, self.num_items)\n",
        "            )\n",
        "\n",
        "        # This just removes the users which were not sampled\n",
        "        return jnp.array(ret[ret.getnnz(1)>0].todense())\n",
        "\n",
        "def load_raw_dataset(dataset, data_path = None, index_path = None):\n",
        "    if data_path is None or index_path is None:\n",
        "        data_path, index_path = [\n",
        "            \"/content/drive/MyDrive/ml-1m{}/total_data.hdf5\".format(dataset),\n",
        "            \"/content/drive/MyDrive/ml-1m{}/index.npz\".format(dataset)\n",
        "        ]\n",
        "\n",
        "    with h5py.File(data_path, 'r') as f: data = np.array(list(zip(f['user'][:], f['item'][:], f['rating'][:])))\n",
        "    index = np.array(np.load(index_path)['data'], dtype = np.int32)\n",
        "\n",
        "    def remap(data, index):\n",
        "        ## Counting number of unique users/items before\n",
        "        valid_users, valid_items = set(), set()\n",
        "        for at, (u, i, r) in enumerate(data):\n",
        "            if index[at] != -1:\n",
        "                valid_users.add(u)\n",
        "                valid_items.add(i)\n",
        "\n",
        "        ## Map creation done!\n",
        "        user_map = dict(zip(list(valid_users), list(range(len(valid_users)))))\n",
        "        item_map = dict(zip(list(valid_items), list(range(len(valid_items)))))\n",
        "\n",
        "        return user_map, item_map\n",
        "\n",
        "    user_map, item_map = remap(data, index)\n",
        "\n",
        "    new_data, new_index = [], []\n",
        "    for at, (u, i, r) in enumerate(data):\n",
        "        if index[at] == -1: continue\n",
        "        new_data.append([ user_map[u], item_map[i], r ])\n",
        "        new_index.append(index[at])\n",
        "    data = np.array(new_data, dtype = np.int32)\n",
        "    index = np.array(new_index, dtype = np.int32)\n",
        "\n",
        "    def select(data, index, index_val):\n",
        "        final = data[np.where(index == index_val)[0]]\n",
        "        final[:, 2] = 1.0\n",
        "        return final.astype(np.int32)\n",
        "\n",
        "    ret = {\n",
        "        'item_map': item_map,\n",
        "        'train':  select(data, index, 0),\n",
        "        'val': select(data, index, 1),\n",
        "        #'test': test_pickle\n",
        "        'test': select(data, index, 2)\n",
        "    }\n",
        "    #print(ret['test'])\n",
        "\n",
        "    num_users = int(max(data[:, 0]) + 1)\n",
        "    num_items = len(item_map)\n",
        "    print(num_users, num_items)\n",
        "\n",
        "    del data, index ; gc.collect()\n",
        "\n",
        "    def make_user_history(arr):\n",
        "        ret = [ set() for _ in range(num_users) ] #[ set() for _ in range(num_users) ]\n",
        "        for u, i, r in arr:\n",
        "            if i >= num_items: continue\n",
        "            ret[int(u)].add(int(i))\n",
        "        return ret\n",
        "\n",
        "    ret['train_positive_set'] = make_user_history(ret['train'])\n",
        "    ret['val_positive_set'] = make_user_history(ret['val'])\n",
        "    ret['test_positive_set'] = make_user_history(ret['test'])\n",
        "\n",
        "    ret['train_matrix'] = csr_matrix(\n",
        "        ( np.ones(ret['train'].shape[0]), (ret['train'][:, 0].astype(np.int32), ret['train'][:, 1].astype(np.int32)) ),\n",
        "        shape = (num_users, num_items)\n",
        "    )\n",
        "\n",
        "    ret['val_matrix'] = csr_matrix(\n",
        "        ( np.ones(ret['val'].shape[0]), (ret['val'][:, 0].astype(np.int32), ret['val'][:, 1].astype(np.int32)) ),\n",
        "        shape = (num_users, num_items)\n",
        "    )\n",
        "\n",
        "    # Negatives will be used for AUC computation\n",
        "    ret['negatives'] = [ set() for _ in range(num_users) ]\n",
        "    for u in range(num_users):\n",
        "        while len(ret['negatives'][u]) < 50:\n",
        "            rand_item = np.random.randint(0, num_items)\n",
        "            if rand_item in ret['train_positive_set'][u]: continue\n",
        "            if rand_item in ret['test_positive_set'][u]: continue\n",
        "            ret['negatives'][u].add(rand_item)\n",
        "        ret['negatives'][u] = list(ret['negatives'][u])\n",
        "    ret['negatives'] = np.array(ret['negatives'], dtype=np.int32)\n",
        "\n",
        "    ret.update({\n",
        "        'num_users': num_users,\n",
        "        'num_items': num_items,\n",
        "        'num_interactions': len(ret['train']),\n",
        "    })\n",
        "    print(\"# users:\", num_users)\n",
        "    print(\"# items:\", num_items)\n",
        "    print(\"# interactions:\", len(ret['train']))\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "FcVxg3SjbOeI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess"
      ],
      "metadata": {
        "id": "NV2GAXtucsJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/ml-1m'\n",
        "#base_path\n",
        "\n",
        "def prep_movielens(ratings_file_path):\n",
        "    f = open(ratings_file_path, \"r\")\n",
        "    users, items, ratings = [], [], []\n",
        "\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        u, i, r,_ = line.strip().split(\"::\")\n",
        "        users.append(int(u))\n",
        "        items.append(int(i))\n",
        "        ratings.append(float(r))\n",
        "        line = f.readline()\n",
        "\n",
        "    min_user = min(users)\n",
        "    num_users = len(set(users))\n",
        "\n",
        "    data = [ [] for _ in range(num_users) ]\n",
        "    for i in range(len(users)):\n",
        "        data[users[i] - min_user].append([ items[i], ratings[i] ])\n",
        "\n",
        "    return rating_data(data)\n",
        "\n",
        "class rating_data:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "        self.index = [] # 0: train, 1: validation, 2: test, -1: removed due to user frequency < 3\n",
        "        for user_data in self.data:\n",
        "            for _ in range(len(user_data)): self.index.append(42)\n",
        "\n",
        "    def train_test_split(self):\n",
        "        at = 0\n",
        "\n",
        "        for user in range(len(self.data)):\n",
        "            first_split_point = int(0.8 * len(self.data[user]))\n",
        "            second_split_point = int(0.9 * len(self.data[user]))\n",
        "\n",
        "            indices = np.arange(len(self.data[user]))\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            for timestep, (item, rating) in enumerate(self.data[user]):\n",
        "                if len(self.data[user]) < 3: self.index[at] = -1\n",
        "                else:\n",
        "                    # Force atleast one element in user history to be in test\n",
        "                    if timestep == indices[0]: self.index[at] = 2\n",
        "                    else:\n",
        "                        if timestep in indices[:first_split_point]: self.index[at] = 0\n",
        "                        elif timestep in indices[first_split_point:second_split_point]: self.index[at] = 1\n",
        "                        else: self.index[at] = 2\n",
        "                at += 1\n",
        "\n",
        "        assert at == len(self.index)\n",
        "        self.complete_data_stats = None\n",
        "\n",
        "    def save_index(self, path):\n",
        "        os.makedirs(path, exist_ok = True)\n",
        "        with open(path + \"/index.npz\", \"wb\") as f: np.savez_compressed(f, data = self.index)\n",
        "\n",
        "    def save_data(self, path):\n",
        "        flat_data = []\n",
        "        for u in range(len(self.data)):\n",
        "            flat_data += list(map(lambda x: [ u ] + x, self.data[u]))\n",
        "        flat_data = np.array(flat_data)\n",
        "\n",
        "        shape = [ len(flat_data) ]\n",
        "\n",
        "        os.makedirs(path, exist_ok = True)\n",
        "        with h5py.File(path + '/total_data.hdf5', 'w') as file:\n",
        "            dset = {}\n",
        "            dset['user'] = file.create_dataset(\"user\", shape, dtype = 'i4', maxshape = shape, compression=\"gzip\")\n",
        "            dset['item'] = file.create_dataset(\"item\", shape, dtype = 'i4', maxshape = shape, compression=\"gzip\")\n",
        "            dset['rating'] = file.create_dataset(\"rating\", shape, dtype = 'f', maxshape = shape, compression=\"gzip\")\n",
        "\n",
        "            dset['user'][:] = flat_data[:, 0]\n",
        "            dset['item'][:] = flat_data[:, 1]\n",
        "            dset['rating'][:] = flat_data[:, 2]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 2: \n",
        "        print(\"This file needs the dataset name as the first argument...\")\n",
        "        exit(0)\n",
        "    \n",
        "    dataset = sys.argv[1]\n",
        "\n",
        "    print(\"\\n\\n!!!!!!!! STARTED PROCESSING {} !!!!!!!!\".format(dataset))\n",
        "\n",
        "    #if dataset in [ 'ml-1m' ]: total_data = prep_movielens(BASE_PATH +\"/ratings.dat\") #\"/ratings.dat\"\n",
        "    total_data = prep_movielens(BASE_PATH +\"/ratings.dat\")\n",
        "\n",
        "    total_data.save_data(BASE_PATH + \"{}/\".format(dataset))\n",
        "    total_data.train_test_split()\n",
        "    total_data.save_index(BASE_PATH + \"{}/\".format(dataset))"
      ],
      "metadata": {
        "id": "eTeAAUEQcqzq",
        "outputId": "7a36f04f-826f-420c-859e-a79566273d0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "!!!!!!!! STARTED PROCESSING -f !!!!!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper_params"
      ],
      "metadata": {
        "id": "0JNE6skYcLGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_params = {\n",
        "\t'dataset': 'ml-1m', \n",
        "\t'float64': False,\n",
        "\n",
        "\t'depth': 1,\n",
        "\t'grid_search_lamda': True,\n",
        "\t'lamda': 1, # Only used if grid_search_lamda == False\n",
        "\n",
        "\t# Number of users to keep (randomly)\n",
        "\t'user_support': -1, # -1 implies use all users\n",
        "\t'seed': 42,\n",
        "}\n"
      ],
      "metadata": {
        "id": "9kIzDX8FcI_w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "dOFlQqjof9r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_kernelized_rr_forward(hyper_params):\n",
        "    _, _, kernel_fn = FullyConnectedNetwork(\n",
        "        depth=hyper_params['depth'],\n",
        "        num_classes=hyper_params['num_items']\n",
        "    )\n",
        "    # NOTE: Un-comment this if the dataset size is very big (didn't need it for experiments in the paper)\n",
        "    # kernel_fn = nt.batch(kernel_fn, batch_size=128)\n",
        "    kernel_fn = functools.partial(kernel_fn, get='ntk')\n",
        "\n",
        "    @jax.jit\n",
        "    def kernelized_rr_forward(X_train, X_predict, reg=0.1):\n",
        "        K_train = kernel_fn(X_train, X_train)\n",
        "        K_predict = kernel_fn(X_predict, X_train)\n",
        "        K_reg = (K_train + jnp.abs(reg) * jnp.trace(K_train) * jnp.eye(K_train.shape[0]) / K_train.shape[0])     \n",
        "        return jnp.dot(K_predict, sp.linalg.solve(K_reg, X_train, sym_pos=True))\n",
        "\n",
        "    return kernelized_rr_forward, kernel_fn\n",
        "\n",
        "def FullyConnectedNetwork( \n",
        "    depth,\n",
        "    W_std = 2 ** 0.5, \n",
        "    b_std = 0.1,\n",
        "    num_classes = 10,\n",
        "    parameterization = 'ntk'\n",
        "):\n",
        "    activation_fn = stax.Relu()\n",
        "    dense = functools.partial(stax.Dense, W_std=W_std, b_std=b_std, parameterization=parameterization)\n",
        "\n",
        "    layers = [stax.Flatten()]\n",
        "    # NOTE: setting width = 1024 doesn't matter as the NTK parameterization will stretch this till \\infty\n",
        "    for _ in range(depth): layers += [dense(1024), activation_fn] \n",
        "    layers += [stax.Dense(num_classes, W_std=W_std, b_std=b_std, parameterization=parameterization)]\n",
        "\n",
        "    return stax.serial(*layers)\n"
      ],
      "metadata": {
        "id": "klyQisKI_63l"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OH616YenB0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate"
      ],
      "metadata": {
        "id": "WzIz4KhLbvzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import jax\n",
        "#import numpy as np\n",
        "#import jax.numpy as jnp\n",
        "#from numba import jit, float64\n",
        "\n",
        "INF = float(1e6)\n",
        "\n",
        "def evaluate(hyper_params, kernelized_rr_forward, data, item_propensity, train_x, topk = [ 10, 100 ], test_set_eval = False):\n",
        "    preds, y_binary, metrics = [], [], {}\n",
        "    for kind in [ 'HR', 'NDCG', 'PSP' ]:\n",
        "        for k in topk: \n",
        "            metrics['{}@{}'.format(kind, k)] = 0.0\n",
        "\n",
        "    # Train positive set -- these items will be set to -infinity while prediction on the val/test set\n",
        "    train_positive_list = list(map(list, data.data['train_positive_set']))\n",
        "    if test_set_eval:\n",
        "        for u in range(len(train_positive_list)): train_positive_list[u] += list(data.data['val_positive_set'][u])\n",
        "\n",
        "    # Train positive interactions (in matrix form) as context for prediction on val/test set\n",
        "    eval_context = data.data['train_matrix']\n",
        "    if test_set_eval: eval_context += data.data['val_matrix']\n",
        "\n",
        "    # What needs to be predicted\n",
        "    to_predict = data.data['val_positive_set']\n",
        "    if test_set_eval: to_predict = data.data['test_positive_set']\n",
        "\n",
        "    bsz = 20_000 # These many users\n",
        "    for i in range(0, hyper_params['num_users'], bsz):\n",
        "        temp_preds = kernelized_rr_forward(train_x, eval_context[i:i+bsz].todense(), reg = hyper_params['lamda'])\n",
        "        #np.save('/content/drive/MyDrive/infinite_ac_cf_main/kernel', temp_preds) # dual parameter save\n",
        "        metrics, temp_preds, temp_y = evaluate_batch(\n",
        "            data.data['negatives'][i:i+bsz], np.array(temp_preds), \n",
        "            train_positive_list[i:i+bsz], to_predict[i:i+bsz], item_propensity, \n",
        "            topk, metrics\n",
        "        )\n",
        "        \n",
        "        preds += temp_preds\n",
        "        y_binary += temp_y\n",
        "\n",
        "    y_binary, preds = np.array(y_binary), np.array(preds)\n",
        "    if (True not in np.isnan(y_binary)) and (True not in np.isnan(preds)):\n",
        "        metrics['AUC'] = round(fast_auc(y_binary, preds), 4)\n",
        "\n",
        "    for kind in [ 'HR', 'NDCG', 'PSP' ]:\n",
        "        for k in topk: \n",
        "            metrics['{}@{}'.format(kind, k)] = round(\n",
        "                float(100.0 * metrics['{}@{}'.format(kind, k)]) / hyper_params['num_users'], 4\n",
        "            )\n",
        "\n",
        "    metrics['num_users'] = int(train_x.shape[0])\n",
        "    metrics['num_interactions'] = int(jnp.count_nonzero(train_x.astype(np.int8)))\n",
        "\n",
        "    return metrics,y_binary,preds\n",
        "\n",
        "def evaluate_batch(auc_negatives, logits, train_positive, test_positive_set, item_propensity, topk, metrics, train_metrics = False):\n",
        "    # AUC Stuff\n",
        "    temp_preds, temp_y = [], []\n",
        "    for b in range(len(logits)):\n",
        "        temp_preds += np.take(logits[b], np.array(list(test_positive_set[b]))).tolist()\n",
        "        temp_y += [ 1.0 for _ in range(len(test_positive_set[b])) ]\n",
        "\n",
        "        temp_preds += np.take(logits[b], auc_negatives[b]).tolist()\n",
        "        temp_y += [ 0.0 for _ in range(len(auc_negatives[b])) ]\n",
        "    # Marking train-set consumed items as negative INF\n",
        "    for b in range(len(logits)): logits[b][ train_positive[b] ] = -INF\n",
        "\n",
        "    indices = (-logits).argsort()[:, :max(topk)].tolist()\n",
        "\n",
        "    for k in topk: \n",
        "        for b in range(len(logits)):\n",
        "            num_pos = float(len(test_positive_set[b]))\n",
        "\n",
        "            metrics['HR@{}'.format(k)] += float(len(set(indices[b][:k]) & test_positive_set[b])) / float(min(num_pos, k))\n",
        "\n",
        "            test_positive_sorted_psp = sorted([ item_propensity[x] for x in test_positive_set[b] ])[::-1]\n",
        "\n",
        "            dcg, idcg, psp, max_psp = 0.0, 0.0, 0.0, 0.0\n",
        "            for at, pred in enumerate(indices[b][:k]):\n",
        "                if pred in test_positive_set[b]: \n",
        "                    dcg += 1.0 / np.log2(at + 2)\n",
        "                    psp += float(item_propensity[pred]) / float(min(num_pos, k))\n",
        "                if at < num_pos: \n",
        "                    idcg += 1.0 / np.log2(at + 2)\n",
        "                    max_psp += test_positive_sorted_psp[at]\n",
        "\n",
        "            metrics['NDCG@{}'.format(k)] += dcg / idcg\n",
        "            metrics['PSP@{}'.format(k)] += psp / max_psp\n",
        "\n",
        "    return metrics, temp_preds, temp_y\n",
        "\n",
        "#@jit(float64(float64[:], float64[:]))\n",
        "def fast_auc(y_true, y_prob):\n",
        "    y_true = y_true[np.argsort(y_prob)]\n",
        "    nfalse, auc = 0, 0\n",
        "    for i in range(len(y_true)):\n",
        "        nfalse += (1 - y_true[i])\n",
        "        auc += y_true[i] * nfalse\n",
        "    return auc / (nfalse * (len(y_true) - nfalse))"
      ],
      "metadata": {
        "id": "W-5lEraFaQMg"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_preds1.shape"
      ],
      "metadata": {
        "id": "fcG5pWHKgKX2",
        "outputId": "86de1dcc-feb1-42ff-8e5b-d3a0de72be20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401692,)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main model"
      ],
      "metadata": {
        "id": "M7x7GT6wdZ2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(hyper_params, data):\n",
        "\n",
        "    # This just instantiates the function\n",
        "    kernelized_rr_forward, kernel_fn = make_kernelized_rr_forward(hyper_params)\n",
        "    #np.save('/content/drive/MyDrive/infinite_ac_cf_main/kernelized_rr_forward', kernelized_rr_forward) # x_save.npy\n",
        "    sampled_matrix = data.sample_users(hyper_params['user_support']) # Random user sample\n",
        "\n",
        "    '''\n",
        "    NOTE: No training required! We will compute dual-variables \\alpha on the fly in `kernelized_rr_forward`\n",
        "          However, if we needed to perform evaluation multiple times, we could pre-compute \\alpha like so:\n",
        "    \n",
        "    import jax, jax.numpy as jnp, jax.scipy as sp\n",
        "    @jax.jit\n",
        "    def precompute_alpha(X, lamda=0.1):\n",
        "        K = kernel_fn(X, X)\n",
        "        K_reg = (K + jnp.abs(lamda) * jnp.trace(K) * jnp.eye(K.shape[0]) / K.shape[0])\n",
        "        return sp.linalg.solve(K_reg, X, sym_pos=True)\n",
        "    alpha = precompute_alpha(sampled_matrix, lamda=0.1) # Change for the desired value of lamda\n",
        "    '''\n",
        "\n",
        "    # Used for computing the PSP-metric\n",
        "    item_propensity = get_item_propensity(hyper_params, data)\n",
        "    \n",
        "    # Evaluation\n",
        "    start_time = time.time()\n",
        "\n",
        "    VAL_METRIC = \"HR@100\"\n",
        "    best_metric, best_lamda = None, None\n",
        "\n",
        "    # Validate on the validation-set\n",
        "    for lamda in [ 0.0, 1.0, 5.0, 20.0, 50.0, 100.0 ] if hyper_params['grid_search_lamda'] else [ hyper_params['lamda'] ]:\n",
        "        hyper_params['lamda'] = lamda\n",
        "        val_metrics,val_y_bi,val_preds = evaluate(hyper_params, kernelized_rr_forward, data, item_propensity, sampled_matrix)\n",
        "        if (best_metric is None) or (val_metrics[VAL_METRIC] > best_metric): best_metric, best_lamda = val_metrics[VAL_METRIC], lamda\n",
        "\n",
        "    # Return metrics with the best lamda on the test-set\n",
        "    hyper_params['lamda'] = best_lamda\n",
        "    test_metrics,test_y_bi,test_preds = evaluate(hyper_params, kernelized_rr_forward, data, item_propensity, sampled_matrix, test_set_eval = True)\n",
        "    \n",
        "    log_end_epoch(hyper_params, test_metrics, 0, time.time() - start_time)\n",
        "    start_time = time.time()\n",
        "\n",
        "    return test_metrics, val_metrics, val_y_bi, val_preds, test_y_bi, test_preds\n",
        "\n",
        "def main(hyper_params, gpu_id = None):\n",
        "    if gpu_id is not None: os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "    if 'float64' in hyper_params and hyper_params['float64'] == True: config.update('jax_enable_x64', True)\n",
        "\n",
        "    np.random.seed(hyper_params['seed'])\n",
        "    random.seed(hyper_params['seed'])\n",
        "\n",
        "    os.makedirs(\"./results/logs/\", exist_ok=True)\n",
        "    hyper_params['log_file'] = \"./results/logs/\" + get_common_path(hyper_params) + \".txt\"\n",
        "    \n",
        "    data = Dataset(hyper_params)\n",
        "    hyper_params = copy.deepcopy(data.hyper_params) # Updated w/ data-stats\n",
        "\n",
        "    return train(hyper_params, data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test, val,val_y_bi1,val_preds1,test_y_bi1,test_preds1 = main(hyper_params)"
      ],
      "metadata": {
        "id": "IxJaizJQbnhL",
        "outputId": "c4dca0f3-004c-4aab-9877-efd6c126d8fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6040 3706\n",
            "# users: 6040\n",
            "# items: 3706\n",
            "# interactions: 791718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| end of step    0 | time = 178.27 | HR@10 = 31.8885 | HR@100 = 60.5264 | NDCG@10 = 33.2327 | NDCG@100 = 42.9653 | PSP@10 = 3.2817 | PSP@100 = 6.6276 | AUC = 0.9456 | num_users = 6040.0000 | num_interactions = 791718.0000 (TEST)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds1.shape"
      ],
      "metadata": {
        "id": "56JLXARidoEy",
        "outputId": "87474ba6-50fd-4f99-ebf6-a8e01448eabc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(410799,)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(test)"
      ],
      "metadata": {
        "id": "5z14jc2mtv3X",
        "outputId": "af109bce-6ead-43f7-f270-d59a4372a709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['HR@10',\n",
              " 'HR@100',\n",
              " 'NDCG@10',\n",
              " 'NDCG@100',\n",
              " 'PSP@10',\n",
              " 'PSP@100',\n",
              " 'AUC',\n",
              " 'num_users',\n",
              " 'num_interactions']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(val)"
      ],
      "metadata": {
        "id": "ifktE3ppumMh",
        "outputId": "ae7bf9d6-38da-4133-d88f-971df3598bd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['HR@10',\n",
              " 'HR@100',\n",
              " 'NDCG@10',\n",
              " 'NDCG@100',\n",
              " 'PSP@10',\n",
              " 'PSP@100',\n",
              " 'AUC',\n",
              " 'num_users',\n",
              " 'num_interactions']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset(hyper_params)"
      ],
      "metadata": {
        "id": "GSD9a_Bfibua",
        "outputId": "6f7133bd-73f1-4d79-b06e-28060fd3fd94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6040 3706\n",
            "# users: 6040\n",
            "# items: 3706\n",
            "# interactions: 791718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.data['train_matrix'].size"
      ],
      "metadata": {
        "id": "U9Mh-dKbib_T",
        "outputId": "a8ab4b58-17c0-4189-f811-589451761865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "791718"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "qJWCGeMgkWY6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(data.data)"
      ],
      "metadata": {
        "id": "Kwqq5txyi-ue",
        "outputId": "818ce40e-2faa-4568-bb32-177a46fe524c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['item_map',\n",
              " 'train',\n",
              " 'val',\n",
              " 'test',\n",
              " 'train_positive_set',\n",
              " 'val_positive_set',\n",
              " 'test_positive_set',\n",
              " 'train_matrix',\n",
              " 'val_matrix',\n",
              " 'negatives',\n",
              " 'num_users',\n",
              " 'num_items',\n",
              " 'num_interactions']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural MF"
      ],
      "metadata": {
        "id": "9hWefEZywNRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loder"
      ],
      "metadata": {
        "id": "R0m-6nnwwSAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loader():\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"\n",
        "        데이터 로드 함수\n",
        "\n",
        "        uids: train user\n",
        "        iids: train item\n",
        "        users: 전체 user\n",
        "        items: 전체 item\n",
        "        df_train\n",
        "        df_test\n",
        "        \"\"\"\n",
        "        # 데이터 로드\n",
        "        file_path = '/content/drive/MyDrive/Neural_CF-master/lastfm-dataset-360K'\n",
        "        df = pd.read_csv(file_path + '/usersha1-artmbid-artname-plays.tsv', delimiter='\\t', header=None)\n",
        "        df = df.drop(df.columns[2], axis=1)\n",
        "        df.columns = ['user', 'item', 'plays']\n",
        "        df = df.dropna()\n",
        "        df = df.loc[df.plays != 0]\n",
        "\n",
        "        # user 샘플링\n",
        "        sample_num = 100000\n",
        "        unique_user_lst = list(np.unique(df['user']))  # 358857명\n",
        "        sample_user_idx = np.random.choice(len(unique_user_lst), sample_num, replace=False)\n",
        "        sample_user_lst = [unique_user_lst[idx] for idx in sample_user_idx]\n",
        "        df = df[df['user'].isin(sample_user_lst)]\n",
        "        df = df.reset_index(drop=True)\n",
        "\n",
        "        # 1명 이상의 artist 데이터가 있는 user 만 사용\n",
        "        df_count = df.groupby(['user']).count()\n",
        "        df['count'] = df.groupby('user')['user'].transform('count')\n",
        "        df = df[df['count'] > 1]\n",
        "\n",
        "        # user, item 아이디 부여\n",
        "        df['user_id'] = df['user'].astype(\"category\").cat.codes\n",
        "        df['item_id'] = df['item'].astype(\"category\").cat.codes\n",
        "\n",
        "        # lookup 테이블 생성\n",
        "        item_lookup = df[['item_id', 'item']].drop_duplicates()\n",
        "        item_lookup['item_id'] = item_lookup.item_id.astype(str)\n",
        "\n",
        "        # train, test 데이터 생성\n",
        "        df = df[['user_id', 'item_id', 'plays']]\n",
        "        df_train, df_test = self.train_test_split(df)\n",
        "\n",
        "        # 전체 user, item 리스트 생성\n",
        "        users = list(np.sort(df.user_id.unique()))\n",
        "        items = list(np.sort(df.item_id.unique()))\n",
        "\n",
        "        # train user, item 리스트 생성\n",
        "        rows = df_train['user_id'].astype(int)\n",
        "        cols = df_train['item_id'].astype(int)\n",
        "        values = list(df_train.plays)\n",
        "\n",
        "        uids = np.array(rows.tolist())\n",
        "        iids = np.array(cols.tolist())\n",
        "\n",
        "        # 각 user 마다 negative item 생성\n",
        "        df_neg = self.get_negatives(uids, iids, items, df_test)\n",
        "\n",
        "        return uids, iids, df_train, df_test, df_neg, users, items, item_lookup\n",
        "\n",
        "    def get_negatives(self, uids, iids, items, df_test):\n",
        "        \"\"\"\n",
        "        negative item 리스트 생성함수\n",
        "        \"\"\"\n",
        "        negativeList = []\n",
        "        test_u = df_test['user_id'].values.tolist()\n",
        "        test_i = df_test['item_id'].values.tolist()\n",
        "\n",
        "        test_ratings = list(zip(test_u, test_i))  # test (user, item)세트\n",
        "        zipped = set(zip(uids, iids))             # train (user, item)세트\n",
        "\n",
        "        for (u, i) in test_ratings:\n",
        "\n",
        "            negatives = []\n",
        "            negatives.append((u, i))\n",
        "            for t in range(100):\n",
        "                j = np.random.randint(len(items))     # neg_item j 1개 샘플링\n",
        "                while (u, j) in zipped:               # j가 train에 있으면 다시뽑고, 없으면 선택\n",
        "                    j = np.random.randint(len(items))\n",
        "                negatives.append(j)\n",
        "            negativeList.append(negatives) # [(0,pos), neg, neg, ...]\n",
        "\n",
        "        df_neg = pd.DataFrame(negativeList)\n",
        "\n",
        "        return df_neg\n",
        "\n",
        "    def mask_first(self, x):\n",
        "\n",
        "        result = np.ones_like(x)\n",
        "        result[0] = 0  # [0,1,1,....]\n",
        "\n",
        "        return result\n",
        "\n",
        "    def train_test_split(self, df):\n",
        "        \"\"\"\n",
        "        train, test 나누는 함수\n",
        "        \"\"\"\n",
        "        df_test = df.copy(deep=True)\n",
        "        df_train = df.copy(deep=True)\n",
        "\n",
        "        # df_test\n",
        "        # user_id와 holdout_item_id(user가 플레이한 아이템 중 1개)뽑기\n",
        "        df_test = df_test.groupby(['user_id']).first()\n",
        "        df_test['user_id'] = df_test.index\n",
        "        df_test = df_test[['user_id', 'item_id', 'plays']]\n",
        "        df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "        # df_train\n",
        "        # user_id 리스트에 make_first()적용\n",
        "        mask = df.groupby(['user_id'])['user_id'].transform(self.mask_first).astype(bool)\n",
        "        df_train = df.loc[mask]\n",
        "\n",
        "        return df_train, df_test\n",
        "\n",
        "    def get_train_instances(self, uids, iids, num_neg, num_items):\n",
        "        \"\"\"\n",
        "        모델에 사용할 train 데이터 생성 함수\n",
        "        \"\"\"\n",
        "        user_input, item_input, labels = [],[],[]\n",
        "        zipped = set(zip(uids, iids)) # train (user, item) 세트\n",
        "\n",
        "        for (u, i) in zip(uids, iids):\n",
        "\n",
        "            # pos item 추가\n",
        "            user_input.append(u)  # [u]\n",
        "            item_input.append(i)  # [pos_i]\n",
        "            labels.append(1)      # [1]\n",
        "\n",
        "            # neg item 추가\n",
        "            for t in range(num_neg):\n",
        "\n",
        "                j = np.random.randint(num_items)      # neg_item j num_neg 개 샘플링\n",
        "                while (u, j) in zipped:               # u가 j를 이미 선택했다면\n",
        "                    j = np.random.randint(num_items)  # 다시 샘플링\n",
        "\n",
        "                user_input.append(u)  # [u1, u1,  u1,  ...]\n",
        "                item_input.append(j)  # [pos_i, neg_j1, neg_j2, ...]\n",
        "                labels.append(0)      # [1, 0,  0,  ...]\n",
        "\n",
        "        return user_input, item_input, labels"
      ],
      "metadata": {
        "id": "kFQ0VDGdkeEx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metric"
      ],
      "metadata": {
        "id": "c5I7vOdiweAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Metric:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_hits(self, k_ranked, holdout):\n",
        "        \"\"\"\n",
        "        hit 생성 함수\n",
        "        hit := holdout(df_test의 item)이 K순위 내에 있는지 여부\n",
        "        \"\"\"\n",
        "        for item in k_ranked:\n",
        "            if item == holdout:\n",
        "                return 1\n",
        "        return 0\n",
        "\n",
        "    def eval_rating(self, idx, test_ratings, test_negatives, K, model):\n",
        "        \"\"\"\n",
        "        holdout(df_test의 item)이 K순위 내에 있는지 평가하는 함수\n",
        "        \"\"\"\n",
        "        items = test_negatives[idx]      # negative items [neg_item_id, ... ] (1,100)\n",
        "        user_idx = test_ratings[idx][0]  # [user_id, item_id][0]\n",
        "        holdout = test_ratings[idx][1]   # [user_id, item_id][1]\n",
        "        items.append(holdout)            # holdout 추가 [neg_item_id, ..., holdout] (1,101)\n",
        "\n",
        "        # prediction\n",
        "        predict_user = np.full(len(items), user_idx, dtype='int32').reshape(-1, 1)  # [[user_id], ...], (101, 1)\n",
        "        np_items = np.array(items).reshape(-1, 1)                                   # [[item_id], ... ], (101, 1)\n",
        "\n",
        "        predictions = model.predict([predict_user, np_items])\n",
        "        predictions = predictions.flatten().tolist()\n",
        "        item_to_pre_score = {item:pre for item, pre in zip(items, predictions)}\n",
        "\n",
        "        # 점수가 높은 상위 k개 아이템 리스트\n",
        "        k_ranked = heapq.nlargest(K, item_to_pre_score, key=item_to_pre_score.get)\n",
        "\n",
        "        # holdout이 상위 K 순위에 포함 되는지 체크\n",
        "        # {1:포함, 0:포함x}\n",
        "        hits = self.get_hits(k_ranked, holdout)\n",
        "\n",
        "        return hits\n",
        "\n",
        "    def evaluate_top_k(self, df_neg, df_test, model, K=10):\n",
        "        \"\"\"\n",
        "        TOP-K metric을 사용해 모델을 평가하는 함수\n",
        "        \"\"\"\n",
        "        hits = []\n",
        "        test_u = df_test['user_id'].values.tolist()\n",
        "        test_i = df_test['item_id'].values.tolist()\n",
        "\n",
        "        test_ratings = list(zip(test_u, test_i))\n",
        "        df_neg = df_neg.drop(df_neg.columns[0], axis=1)\n",
        "        test_negatives = df_neg.values.tolist()  # [[(user_id, item_id=holdout)], neg_item,... ] (1,100)\n",
        "\n",
        "        # user 샘플링\n",
        "        sample_idx_lst = np.random.choice(len(test_ratings), int(len(test_ratings) * 0.3))\n",
        "        for user_idx in sample_idx_lst:  # 전체 사용: range(len(test_ratings))\n",
        "\n",
        "            hitrate = self.eval_rating(user_idx, test_ratings, test_negatives, K, model)\n",
        "            hits.append(hitrate)  # ex. [1,0,1,1,0,...] (1, df_test.shape[0])\n",
        "\n",
        "        return hits\n"
      ],
      "metadata": {
        "id": "s0H_7LgBwdgh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GMF"
      ],
      "metadata": {
        "id": "BvNc1F1owsWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GMP:\n",
        "\n",
        "    def __init__(self, user_num, item_num):\n",
        "\n",
        "        latent_features = 8\n",
        "\n",
        "        # User embedding\n",
        "        user = Input(shape=(1,), dtype='int32')\n",
        "        user_embedding = Embedding(user_num, latent_features, input_length=user.shape[1])(user)\n",
        "        user_embedding = Flatten()(user_embedding)\n",
        "\n",
        "        # Item embedding\n",
        "        item = Input(shape=(1,), dtype='int32')\n",
        "        item_embedding = Embedding(item_num, latent_features, input_length=item.shape[1])(item)\n",
        "        item_embedding = Flatten()(item_embedding)\n",
        "\n",
        "        # Merge\n",
        "        concatenated = Multiply()([user_embedding, item_embedding])\n",
        "\n",
        "        # Output\n",
        "        output_layer = Dense(1, kernel_initializer='lecun_uniform', name='output_layer')(concatenated) # 1,1 / h(8,1)초기화\n",
        "\n",
        "        # Model\n",
        "        self.model = Model([user, item], output_layer)\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    def get_model(self):\n",
        "        model = self.model\n",
        "        return model"
      ],
      "metadata": {
        "id": "kXBIOtFOwTvS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "an8UH4K0w7-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "\n",
        "    def __init__(self, user_num, item_num):\n",
        "\n",
        "        # User embedding\n",
        "        user = Input(shape=(1,), dtype='int32')\n",
        "        user_embedding = Embedding(user_num, 32, input_length=user.shape[1])(user)\n",
        "        user_embedding = Flatten()(user_embedding)\n",
        "\n",
        "        # Item embedding\n",
        "        item = Input(shape=(1,), dtype='int32')\n",
        "        item_embedding = Embedding(item_num, 32, input_length=item.shape[1])(item)\n",
        "        item_embedding = Flatten()(item_embedding)\n",
        "\n",
        "        # Merge\n",
        "        concatenated = Concatenate()([user_embedding, item_embedding])\n",
        "        dropout = Dropout(rate=0.2)(concatenated)\n",
        "\n",
        "        # Layer1\n",
        "        layer_1 = Dense(units=64, activation='relu', name='layer1')(dropout)  # (64,1)\n",
        "        dropout1 = Dropout(rate=0.2, name='dropout1')(layer_1)                # (64,1)\n",
        "        batch_norm1 = BatchNormalization(name='batch_norm1')(dropout1)        # (64,1)\n",
        "\n",
        "        # Layer2\n",
        "        layer_2 = Dense(units=32, activation='relu', name='layer2')(batch_norm1)  # (32,1)\n",
        "        dropout2 = Dropout(rate=0.2, name='dropout2')(layer_2)                    # (32,1)\n",
        "        batch_norm2 = BatchNormalization(name='batch_norm2')(dropout2)            # (32,1)\n",
        "\n",
        "        # Layer3\n",
        "        layer_3 = Dense(units=16, activation='relu', name='layer3')(batch_norm2)  # (16,1)\n",
        "\n",
        "        # Layer4\n",
        "        layer_4 = Dense(units=8, activation='relu', name='layer4')(layer_3)  # (8,1)\n",
        "\n",
        "        # Output\n",
        "        output_layer = Dense(1, kernel_initializer='lecun_uniform', name='output_layer')(layer_4)  # (1,1) / h(8,1)초기화\n",
        "\n",
        "        # Model\n",
        "        self.model = Model([user, item], output_layer)\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    def get_model(self):\n",
        "        model = self.model\n",
        "        return model"
      ],
      "metadata": {
        "id": "RDgH94fqwu-u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NeuralMF"
      ],
      "metadata": {
        "id": "xmlBuuDAw-4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuMF:\n",
        "\n",
        "    def __init__(self, user_num, item_num):\n",
        "\n",
        "        latent_features = 8\n",
        "\n",
        "        # Input\n",
        "        user = Input(shape=(1,), dtype='int32')\n",
        "        item = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # User embedding for GMF\n",
        "        gmf_user_embedding = Embedding(user_num, latent_features, input_length=user.shape[1])(user)\n",
        "        gmf_user_embedding = Flatten()(gmf_user_embedding)\n",
        "\n",
        "        # Item embedding for GMF\n",
        "        gmf_item_embedding = Embedding(item_num, latent_features, input_length=item.shape[1])(item)\n",
        "        gmf_item_embedding = Flatten()(gmf_item_embedding)\n",
        "\n",
        "        # User embedding for MLP\n",
        "        mlp_user_embedding = Embedding(user_num, 32, input_length=user.shape[1])(user)\n",
        "        mlp_user_embedding = Flatten()(mlp_user_embedding)\n",
        "\n",
        "        # Item embedding for MLP\n",
        "        mlp_item_embedding = Embedding(item_num, 32, input_length=item.shape[1])(item)\n",
        "        mlp_item_embedding = Flatten()(mlp_item_embedding)\n",
        "\n",
        "        # GMF layers\n",
        "        gmf_mul =  Multiply()([gmf_user_embedding, gmf_item_embedding])\n",
        "\n",
        "        # MLP layers\n",
        "        mlp_concat = Concatenate()([mlp_user_embedding, mlp_item_embedding])\n",
        "        mlp_dropout = Dropout(0.2)(mlp_concat)\n",
        "\n",
        "        # Layer1\n",
        "        mlp_layer_1 = Dense(units=64, activation='relu', name='mlp_layer1')(mlp_dropout)  # (64,1)\n",
        "        mlp_dropout1 = Dropout(rate=0.2, name='dropout1')(mlp_layer_1)                    # (64,1)\n",
        "        mlp_batch_norm1 = BatchNormalization(name='batch_norm1')(mlp_dropout1)            # (64,1)\n",
        "\n",
        "        # Layer2\n",
        "        mlp_layer_2 = Dense(units=32, activation='relu', name='mlp_layer2')(mlp_batch_norm1)  # (32,1)\n",
        "        mlp_dropout2 = Dropout(rate=0.2, name='dropout2')(mlp_layer_2)                        # (32,1)\n",
        "        mlp_batch_norm2 = BatchNormalization(name='batch_norm2')(mlp_dropout2)                # (32,1)\n",
        "\n",
        "        # Layer3\n",
        "        mlp_layer_3 = Dense(units=16, activation='relu', name='mlp_layer3')(mlp_batch_norm2)  # (16,1)\n",
        "\n",
        "        # Layer4\n",
        "        mlp_layer_4 = Dense(units=8, activation='relu', name='mlp_layer4')(mlp_layer_3)       # (8,1)\n",
        "\n",
        "        # merge GMF + MLP\n",
        "        merged_vector = tf.keras.layers.concatenate([gmf_mul, mlp_layer_4])\n",
        "        \n",
        "        # Output layer\n",
        "        output_layer = Dense(1, kernel_initializer='lecun_uniform', name='output_layer')(merged_vector) # 1,1 / h(8,1)초기화\n",
        "\n",
        "        # Model\n",
        "        self.model = Model([user, item], output_layer)\n",
        "        self.model.compile(optimizer= 'adam', loss= 'binary_crossentropy')\n",
        "\n",
        "    def get_model(self):\n",
        "        model = self.model\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "YH3uMTufw6Wx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EMWzTuicp_iZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Run:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # data 로드\n",
        "        loader = Loader()\n",
        "\n",
        "        print('start data load..')\n",
        "\n",
        "        num_neg = 4\n",
        "        uids, iids, self.df_train, self.df_test, \\\n",
        "        self.df_neg, self.users, self.items, item_lookup = loader.load_dataset()\n",
        "        user_input, item_input, labels = loader.get_train_instances(uids, iids, num_neg, len(self.items))\n",
        "\n",
        "        print('end data load..')\n",
        "\n",
        "        # input data 준비\n",
        "        user_data_shuff, item_data_shuff, label_data_shuff = shuffle(user_input, item_input, labels)\n",
        "        self.user_data_shuff = np.array(user_data_shuff).reshape(-1,1)\n",
        "        self.item_data_shuff = np.array(item_data_shuff).reshape(-1,1)\n",
        "        self.label_data_shuff = np.array(label_data_shuff).reshape(-1,1)\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        nmf = GMP(len(self.users), len(self.items))  # Neural Collaborative Filtering\n",
        "        self.model = nmf.get_model()\n",
        "        self.model.fit([self.user_data_shuff, self.item_data_shuff], self.label_data_shuff, epochs=20,\n",
        "                       batch_size=256, verbose=1)\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def calculate_top_k_metric(self):\n",
        "        metric = Metric()\n",
        "        hit_lst = metric.evaluate_top_k(self.df_neg, self.df_test, self.model, K=10)\n",
        "        hit = np.mean(hit_lst)\n",
        "\n",
        "        return hit\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    ncf = Run()\n",
        "    model = ncf.run()\n",
        "\n",
        "    # top-k metric\n",
        "    top_k_metric = ncf.calculate_top_k_metric()\n",
        "    print('metric:', top_k_metric)\n",
        "\n",
        "    # user 한 명에 대한 prediction 예시\n",
        "    user_id = 0\n",
        "    user_candidate_item = np.array([134, 6783, 27888, 8362, 25]).reshape(-1, 1)\n",
        "    user_input = np.full(len(user_candidate_item), user_id, dtype='int32').reshape(-1, 1)\n",
        "\n",
        "    predictions = model.predict([user_input, user_candidate_item])\n",
        "    predictions = predictions.flatten().tolist()\n",
        "    item_to_pre_score = {item[0]: pre for item, pre in zip(user_candidate_item, predictions)}  # 후보 아이템 별 예측값\n",
        "    item_to_pre_score = dict(sorted(item_to_pre_score.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "    recommend_item_lst = list(item_to_pre_score.keys())\n",
        "    print('recommend:', recommend_item_lst)"
      ],
      "metadata": {
        "id": "dWSAU1a4xDov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/neural_collaborative_filtering-master"
      ],
      "metadata": {
        "id": "BNnuQFWDv2oH",
        "outputId": "dc7c1e43-393f-4e70-dd53-231675202853",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/neural_collaborative_filtering-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "NJaQtRgDv4j_",
        "outputId": "871bd065-46b0-42cf-a596-0bf589d9b11c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/neural_collaborative_filtering-master'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Theano==0.8.0rc1"
      ],
      "metadata": {
        "id": "5FsFXIJz9PQ2",
        "outputId": "0ff39832-e86e-4a47-9876-290482dadc7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Theano==0.8.0rc1\n",
            "  Downloading Theano-0.8.0rc1.zip (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from Theano==0.8.0rc1) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.11 in /usr/local/lib/python3.8/dist-packages (from Theano==0.8.0rc1) (1.7.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from Theano==0.8.0rc1) (1.15.0)\n",
            "Building wheels for collected packages: Theano\n",
            "  Building wheel for Theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Theano: filename=Theano-0.8.0rc1-py3-none-any.whl size=2715325 sha256=43bbde2f41ce0ceca90e86a26d3e9ed5b96d0926bb6ec42b3c27f092658196f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/6b/4b/24bcef69f8eed3707c8627483b28a1c94373407fcf4e27ff2d\n",
            "Successfully built Theano\n",
            "Installing collected packages: Theano\n",
            "Successfully installed Theano-0.8.0rc1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python GMF.py --dataset ml-1m --epochs 10 --batch_size 256 --num_factors 8 --regs [0,0] --num_neg 4 --lr 0.001 --learner adam --verbose 1 --out 1"
      ],
      "metadata": {
        "id": "9xULlfbJCSe_",
        "outputId": "243535d3-9493-4bb0-b5e4-9fef40d6f474",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-21 20:20:56.756954: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-21 20:20:57.967332: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-21 20:20:57.967425: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-21 20:20:57.967445: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "GMF arguments: Namespace(batch_size=256, dataset='ml-1m', epochs=10, learner='adam', lr=0.001, num_factors=8, num_neg=4, out=1, path='Data/', regs='[0,0]', verbose=1)\n",
            "Load data done [11.1 s]. #user=6040, #item=3706, #train=994169, #test=6040\n",
            "2023-02-21 20:21:10.855354: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "Init: HR = 0.0939, NDCG = 0.0420\t [270.2 s]\n",
            "Iteration 0 [37.8 s]: HR = 0.4543, NDCG = 0.2519, loss = 0.3667 [268.9 s]\n",
            "Iteration 1 [55.3 s]: HR = 0.4526, NDCG = 0.2516, loss = 0.3587 [270.0 s]\n",
            "Iteration 2 [38.1 s]: HR = 0.4500, NDCG = 0.2496, loss = 0.3581 [268.8 s]\n",
            "Iteration 3 [37.8 s]: HR = 0.4508, NDCG = 0.2505, loss = 0.3577 [278.9 s]\n",
            "Iteration 4 [55.1 s]: HR = 0.4505, NDCG = 0.2510, loss = 0.3575 [278.2 s]\n",
            "Iteration 5 [55.3 s]: HR = 0.4493, NDCG = 0.2509, loss = 0.3574 [281.3 s]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "ElB4PGhWTQC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on Aug 8, 2016\n",
        "Processing datasets. \n",
        "\n",
        "@author: Xiangnan He (xiangnanhe@gmail.com)\n",
        "'''\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "\n",
        "class Dataset(object):\n",
        "    '''\n",
        "    classdocs\n",
        "    '''\n",
        "\n",
        "    def __init__(self, path):\n",
        "        '''\n",
        "        Constructor\n",
        "        '''\n",
        "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
        "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
        "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
        "        assert len(self.testRatings) == len(self.testNegatives)\n",
        "        \n",
        "        self.num_users, self.num_items = self.trainMatrix.shape\n",
        "        \n",
        "    def load_rating_file_as_list(self, filename):\n",
        "        ratingList = []\n",
        "        with open(filename, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item = int(arr[0]), int(arr[1])\n",
        "                ratingList.append([user, item])\n",
        "                line = f.readline()\n",
        "        return ratingList\n",
        "    \n",
        "    def load_negative_file(self, filename):\n",
        "        negativeList = []\n",
        "        with open(filename, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                negatives = []\n",
        "                for x in arr[1: ]:\n",
        "                    negatives.append(int(x))\n",
        "                negativeList.append(negatives)\n",
        "                line = f.readline()\n",
        "        return negativeList\n",
        "    \n",
        "    def load_rating_file_as_matrix(self, filename):\n",
        "        '''\n",
        "        Read .rating file and Return dok matrix.\n",
        "        The first line of .rating file is: num_users\\t num_items\n",
        "        '''\n",
        "        # Get number of users and items\n",
        "        num_users, num_items = 0, 0\n",
        "        with open(filename, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                u, i = int(arr[0]), int(arr[1])\n",
        "                num_users = max(num_users, u)\n",
        "                num_items = max(num_items, i)\n",
        "                line = f.readline()\n",
        "        # Construct matrix\n",
        "        mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
        "        with open(filename, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "                if (rating > 0):\n",
        "                    mat[user, item] = 1.0\n",
        "                line = f.readline()    \n",
        "        return mat"
      ],
      "metadata": {
        "id": "7MdiWLA_S10g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate"
      ],
      "metadata": {
        "id": "9gRhJN7WS0M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import heapq # for retrieval topK\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "from time import time\n",
        "#from numba import jit, autojit\n",
        "\n",
        "# Global variables that are shared across processes\n",
        "_model = None\n",
        "_testRatings = None\n",
        "_testNegatives = None\n",
        "_K = None\n",
        "\n",
        "def evaluate_model(model, testRatings, testNegatives, K, num_thread):\n",
        "    \"\"\"\n",
        "    Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
        "    Return: score of each test rating.\n",
        "    \"\"\"\n",
        "    global _model\n",
        "    global _testRatings\n",
        "    global _testNegatives\n",
        "    global _K\n",
        "    _model = model\n",
        "    _testRatings = testRatings\n",
        "    _testNegatives = testNegatives\n",
        "    _K = K\n",
        "        \n",
        "    hits, ndcgs = [],[]\n",
        "    if(num_thread > 1): # Multi-thread\n",
        "        pool = multiprocessing.Pool(processes=num_thread)\n",
        "        res = pool.map(eval_one_rating, range(len(_testRatings)))\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        hits = [r[0] for r in res]\n",
        "        ndcgs = [r[1] for r in res]\n",
        "        return (hits, ndcgs)\n",
        "    # Single thread\n",
        "    for idx in range(len(_testRatings)):\n",
        "        (hr,ndcg) = eval_one_rating(idx)\n",
        "        hits.append(hr)\n",
        "        ndcgs.append(ndcg)      \n",
        "    return (hits, ndcgs)\n",
        "\n",
        "def eval_one_rating(idx):\n",
        "    rating = _testRatings[idx]\n",
        "    items = _testNegatives[idx]\n",
        "    u = rating[0]\n",
        "    gtItem = rating[1]\n",
        "    items.append(gtItem)\n",
        "    # Get prediction scores\n",
        "    map_item_score = {}\n",
        "    users = np.full(len(items), u, dtype = 'int32')\n",
        "    predictions = _model.predict([users, np.array(items)], \n",
        "                                 batch_size=100, verbose=0)\n",
        "    for i in range(len(items)):\n",
        "        item = items[i]\n",
        "        map_item_score[item] = predictions[i]\n",
        "    items.pop()\n",
        "    \n",
        "    # Evaluate top rank list\n",
        "    ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
        "    hr = getHitRatio(ranklist, gtItem)\n",
        "    ndcg = getNDCG(ranklist, gtItem)\n",
        "    return (hr, ndcg)\n",
        "\n",
        "def getHitRatio(ranklist, gtItem):\n",
        "    for item in ranklist:\n",
        "        if item == gtItem:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def getNDCG(ranklist, gtItem):\n",
        "    for i in range(len(ranklist)):\n",
        "        item = ranklist[i]\n",
        "        if item == gtItem:\n",
        "            return math.log(2) / math.log(i+2)\n",
        "    return 0\n"
      ],
      "metadata": {
        "id": "6paXI12aSxnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GMF"
      ],
      "metadata": {
        "id": "jA0_JhhkSn0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import theano.tensor as T\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "from keras.models import Sequential, Model, load_model, save_model\n",
        "from keras.layers.core import Dense, Lambda, Activation\n",
        "from keras.layers import Embedding, Input, Dense, Reshape, Flatten, Concatenate\n",
        "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
        "from keras.regularizers import l2\n",
        "from Dataset import Dataset\n",
        "from evaluate import evaluate_model\n",
        "from time import time\n",
        "import multiprocessing as mp\n",
        "import sys\n",
        "import math\n",
        "import argparse\n",
        "\n",
        "#################### Arguments ####################\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Run GMF.\")\n",
        "    parser.add_argument('--path', nargs='?', default='Data/',\n",
        "                        help='Input data path.')\n",
        "    parser.add_argument('--dataset', nargs='?', default='ml-1m',\n",
        "                        help='Choose a dataset.')\n",
        "    parser.add_argument('--epochs', type=int, default=100,\n",
        "                        help='Number of epochs.')\n",
        "    parser.add_argument('--batch_size', type=int, default=256,\n",
        "                        help='Batch size.')\n",
        "    parser.add_argument('--num_factors', type=int, default=8,\n",
        "                        help='Embedding size.')\n",
        "    parser.add_argument('--regs', nargs='?', default='[0,0]',\n",
        "                        help=\"Regularization for user and item embeddings.\")\n",
        "    parser.add_argument('--num_neg', type=int, default=4,\n",
        "                        help='Number of negative instances to pair with a positive instance.')\n",
        "    parser.add_argument('--lr', type=float, default=0.001,\n",
        "                        help='Learning rate.')\n",
        "    parser.add_argument('--learner', nargs='?', default='adam',\n",
        "                        help='Specify an optimizer: adagrad, adam, rmsprop, sgd')\n",
        "    parser.add_argument('--verbose', type=int, default=1,\n",
        "                        help='Show performance per X iterations')\n",
        "    parser.add_argument('--out', type=int, default=1,\n",
        "                        help='Whether to save the trained model.')\n",
        "    return parser.parse_args()\n",
        "\n",
        "def init_normal(shape, name=None):\n",
        "    return initializations.normal(shape, scale=0.01, name=name)\n",
        "\n",
        "def get_model(num_users, num_items, latent_dim, regs=[0,0]):\n",
        "    # Input variables\n",
        "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
        "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
        "\n",
        "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
        "                                  embeddings_initializer=\"uniform\", embeddings_regularizer = l2(regs[0]), input_length=1)\n",
        "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
        "                                  embeddings_initializer=\"uniform\", embeddings_regularizer = l2(regs[1]), input_length=1)   \n",
        "    \n",
        "    # Crucial to flatten an embedding vector!\n",
        "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
        "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
        "    \n",
        "    # Element-wise product of user and item embeddings Concatenate()([x1, x2])\n",
        "    predict_vector = Concatenate()([user_latent, item_latent]) #이부분을 바꿔야 함\n",
        "    #Multiply()([user_latent, item_latent])\n",
        "    \n",
        "    # Final prediction layer\n",
        "    #prediction = Lambda(lambda x: K.sigmoid(K.sum(x)), output_shape=(1,))(predict_vector)\n",
        "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n",
        "    \n",
        "    model = Model([user_input, item_input], prediction)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_train_instances(train, num_negatives):\n",
        "    user_input, item_input, labels = [],[],[]\n",
        "    num_users = train.shape[0]\n",
        "    for (u, i) in train.keys():\n",
        "        # positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        labels.append(1)\n",
        "        # negative instances\n",
        "        for t in range(num_negatives):\n",
        "            j = np.random.randint(num_items)\n",
        "            while train.__contains__((u, j)):\n",
        "                j = np.random.randint(num_items)\n",
        "            user_input.append(u)\n",
        "            item_input.append(j)\n",
        "            labels.append(0)\n",
        "    return user_input, item_input, labels\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parse_args()\n",
        "    num_factors = args.num_factors\n",
        "    regs = eval(args.regs)\n",
        "    num_negatives = args.num_neg\n",
        "    learner = args.learner\n",
        "    learning_rate = args.lr\n",
        "    epochs = args.epochs\n",
        "    batch_size = args.batch_size\n",
        "    verbose = args.verbose\n",
        "    \n",
        "    topK = 10\n",
        "    evaluation_threads = 1 #mp.cpu_count()\n",
        "    print(\"GMF arguments: %s\" %(args))\n",
        "    model_out_file = 'Pretrain/%s_GMF_%d_%d.h5' %(args.dataset, num_factors, time())\n",
        "    \n",
        "    # Loading data\n",
        "    t1 = time()\n",
        "    dataset = Dataset(args.path + args.dataset)\n",
        "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
        "          %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
        "    \n",
        "    # Build model\n",
        "    model = get_model(num_users, num_items, num_factors, regs)\n",
        "    if learner.lower() == \"adagrad\": \n",
        "        model.compile(optimizer=Adagrad(lr=learning_rate), loss='binary_crossentropy')\n",
        "    elif learner.lower() == \"rmsprop\":\n",
        "        model.compile(optimizer=RMSprop(lr=learning_rate), loss='binary_crossentropy')\n",
        "    elif learner.lower() == \"adam\":\n",
        "        model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
        "    else:\n",
        "        model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy')\n",
        "    #print(model.summary())\n",
        "    \n",
        "    # Init performance\n",
        "    t1 = time()\n",
        "    (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
        "    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
        "    #mf_embedding_norm = np.linalg.norm(model.get_layer('user_embedding').get_weights())+np.linalg.norm(model.get_layer('item_embedding').get_weights())\n",
        "    #p_norm = np.linalg.norm(model.get_layer('prediction').get_weights()[0])\n",
        "    print('Init: HR = %.4f, NDCG = %.4f\\t [%.1f s]' % (hr, ndcg, time()-t1))\n",
        "    \n",
        "    # Train model\n",
        "    best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
        "    for epoch in range(epochs):\n",
        "        t1 = time()\n",
        "        # Generate training instances\n",
        "        user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
        "        \n",
        "        # Training\n",
        "        hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
        "                         np.array(labels), # labels \n",
        "                         batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "        t2 = time()\n",
        "        \n",
        "        # Evaluation\n",
        "        if epoch %verbose == 0:\n",
        "            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
        "            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
        "            print('Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' \n",
        "                  % (epoch,  t2-t1, hr, ndcg, loss, time()-t2))\n",
        "            if hr > best_hr:\n",
        "                best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
        "                if args.out > 0:\n",
        "                    model.save_weights(model_out_file, overwrite=True)\n",
        "\n",
        "    print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))\n",
        "    if args.out > 0:\n",
        "        print(\"The best GMF model is saved to %s\" %(model_out_file))"
      ],
      "metadata": {
        "id": "DVfRZ84fSbdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_kernelized_rr_forward(hyper_params):\n",
        "    _, _, kernel_fn = FullyConnectedNetwork(\n",
        "        depth=hyper_params['depth'],\n",
        "        num_classes=hyper_params['num_items']\n",
        "    )\n",
        "    # NOTE: Un-comment this if the dataset size is very big (didn't need it for experiments in the paper)\n",
        "    # kernel_fn = nt.batch(kernel_fn, batch_size=128)\n",
        "    kernel_fn = functools.partial(kernel_fn, get='ntk')\n",
        "\n",
        "    @jax.jit\n",
        "    def kernelized_rr_forward(X_train, X_predict, reg=0.1):\n",
        "        K_train = kernel_fn(X_train, X_train)\n",
        "        K_predict = kernel_fn(X_predict, X_train)\n",
        "        K_reg = (K_train + jnp.abs(reg) * jnp.trace(K_train) * jnp.eye(K_train.shape[0]) / K_train.shape[0])     \n",
        "        return jnp.dot(K_predict, sp.linalg.solve(K_reg, X_train, sym_pos=True))\n",
        "\n",
        "    return kernelized_rr_forward, kernel_fn"
      ],
      "metadata": {
        "id": "h-3RxwCjEsjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax, jax.numpy as jnp, jax.scipy as sp\n",
        "@jax.jit\n",
        "def precompute_alpha(X, lamda=0.1):\n",
        "      K = kernel_fn(X, X)\n",
        "      K_reg = (K + jnp.abs(lamda) * jnp.trace(K) * jnp.eye(K.shape[0]) / K.shape[0])\n",
        "      return sp.linalg.solve(K_reg, X, sym_pos=True)\n",
        "alpha = precompute_alpha(sampled_matrix, lamda=0.1) # Change for the desired value of lamda"
      ],
      "metadata": {
        "id": "q2zOpIchJYld"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/infinite_ac_cf_main/dual.npy', alpha)"
      ],
      "metadata": {
        "id": "_2GIs9KW0bIC"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data.pickle','wb') as fw:\n",
        "    pickle.dump(data, fw)"
      ],
      "metadata": {
        "id": "4LerCPtO38Fe"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('hyper_params.pickle','wb') as fw:\n",
        "    pickle.dump(hyper_params, fw)"
      ],
      "metadata": {
        "id": "PF8MfoqR6VUr"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open( \"my_pickle\", \"wb\" ) as file:\n",
        "    pickle.dump( data, file )"
      ],
      "metadata": {
        "id": "F5vvLEzz3qsr"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(data.data)"
      ],
      "metadata": {
        "id": "-qRdbw5X0wul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_params"
      ],
      "metadata": {
        "id": "zxFJzm273Xu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/infinite_ac_cf_main/sample_matrix.npy', sampled_matrix)"
      ],
      "metadata": {
        "id": "nLKbrORH0COP"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(alpha)"
      ],
      "metadata": {
        "id": "uk0J2EfqzMJm",
        "outputId": "accbfbf3-237a-4bf5-8dbb-c2c95a77b209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.03849338, -0.5259235 ,  0.07748814, ...,  0.32294863,\n",
              "         0.05692589, -0.16924861],\n",
              "       [-0.01344344, -0.77470976, -0.48586833, ..., -0.08762714,\n",
              "         0.3353812 , -0.1459166 ],\n",
              "       [-1.209899  , -0.07146695,  0.4709584 , ..., -0.08826166,\n",
              "         0.06534959,  0.41361025],\n",
              "       ...,\n",
              "       [-0.10707396,  0.86223257, -0.08840517, ..., -0.18170378,\n",
              "         0.00143642, -0.644153  ],\n",
              "       [-0.22544856,  0.29969382, -0.24683109, ..., -0.6205665 ,\n",
              "         0.06600394, -0.3530481 ],\n",
              "       [-0.14205636, -0.02629775, -0.26154983, ...,  0.23878607,\n",
              "         0.01149321,  0.03879409]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_matrix = data.sample_users(hyper_params['user_support'])"
      ],
      "metadata": {
        "id": "36q3QNLT7QC7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kernelized_rr_forward(X_train, X_predict, reg=0.1):\n",
        "    K_train = kernel_fn(X_train, X_train)\n",
        "    K_predict = kernel_fn(X_predict, X_train)\n",
        "    K_reg = (K_train + jnp.abs(reg) * jnp.trace(K_train) * jnp.eye(K_train.shape[0]) / K_train.shape[0])     \n",
        "    return jnp.dot(K_predict, sp.linalg.solve(K_reg, X_train, sym_pos=True))"
      ],
      "metadata": {
        "id": "n9krlRbn7XXp"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_matrix.shape"
      ],
      "metadata": {
        "id": "0G31yzNu7e8A",
        "outputId": "d5c58935-f1e4-431c-afb4-9cbd879cd952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6040, 3706)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K = kernel_fn(sampled_matrix, sampled_matrix)"
      ],
      "metadata": {
        "id": "4BAhudLcxMZd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bsz = 20_000 # These many users\n",
        "for i in range(0, 6040, bsz):\n",
        "    temp_preds = kernelized_rr_forward(sampled_matrix, eval_context[i:i+bsz].todense(), reg = hyper_params['lamda'])\n",
        "    #np.save('/content/drive/MyDrive/infinite_ac_cf_main/kernel', temp_preds) # dual parameter save\n",
        "    metrics, temp_preds, temp_y = evaluate_batch(\n",
        "        data.data['negatives'][i:i+bsz], np.array(temp_preds), \n",
        "        train_positive_list[i:i+bsz], to_predict[i:i+bsz], item_propensity, \n",
        "        topk, metrics)\n",
        "        \n",
        "    preds += temp_preds\n",
        "    y_binary += temp_y\n",
        "\n",
        "y_binary, preds = np.array(y_binary), np.array(preds)"
      ],
      "metadata": {
        "id": "yF3pAsiclBdG",
        "outputId": "c6608b14-167c-4261-dae6-e9f905977ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-ca1d90cce546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6040\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtemp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernelized_rr_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lamda'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#np.save('/content/drive/MyDrive/infinite_ac_cf_main/kernel', temp_preds) # dual parameter save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-92-1f85ac75898d>\u001b[0m in \u001b[0;36mkernelized_rr_forward\u001b[0;34m(X_train, X_predict, reg)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mK_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mK_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mK_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosed_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     out_flat = xla.xla_call(\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1682\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1694\u001b[0m   \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0minline\u001b[0m  \u001b[0;31m# Only used at tracing time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m   compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,\n\u001b[0m\u001b[1;32m    143\u001b[0m                                *unsafe_map(arg_spec, args))\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_xla_callable_uncached\u001b[0;34m(fun, device, backend, name, donated_invars, *arg_specs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                            donated_invars, *arg_specs):\n\u001b[0;32m--> 169\u001b[0;31m   return lower_xla_callable(fun, device, backend, name, donated_invars,\n\u001b[0m\u001b[1;32m    170\u001b[0m                             *arg_specs).compile().unsafe_call\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mlower_xla_callable\u001b[0;34m(fun, device, backend, name, donated_invars, *arg_specs)\u001b[0m\n\u001b[1;32m    196\u001b[0m                         \"for jit in {elapsed_time} sec\"):\n\u001b[0;32m--> 197\u001b[0;31m     jaxpr, out_avals, consts = pe.trace_to_jaxpr_final(\n\u001b[0m\u001b[1;32m    198\u001b[0m         fun, abstract_args, pe.debug_info_final(fun, \"jit\"))\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_final\u001b[0;34m(fun, in_avals, debug_info)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1680\u001b[0;31m       \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1681\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals, keep_inputs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[0min_tracers_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1658\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(a, offset, axis1, axis2, dtype, out)\u001b[0m\n\u001b[1;32m   4417\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4418\u001b[0;31m   \u001b[0m_check_arraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4419\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_check_arraylike\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{} requires ndarray or scalar arguments, got {} at position {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: TypeError: trace requires ndarray or scalar arguments, got <class 'neural_tangents._src.utils.kernel.Kernel'> at position 0.\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-ca1d90cce546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbsz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20_000\u001b[0m \u001b[0;31m# These many users\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6040\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtemp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernelized_rr_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lamda'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#np.save('/content/drive/MyDrive/infinite_ac_cf_main/kernel', temp_preds) # dual parameter save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     metrics, temp_preds, temp_y = evaluate_batch(\n",
            "\u001b[0;32m<ipython-input-92-1f85ac75898d>\u001b[0m in \u001b[0;36mkernelized_rr_forward\u001b[0;34m(X_train, X_predict, reg)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mK_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mK_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mK_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mK_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(a, offset, axis1, axis2, dtype, out)\u001b[0m\n\u001b[1;32m   4416\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_argnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'offset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'axis1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'axis2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4417\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4418\u001b[0;31m   \u001b[0m_check_arraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4419\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4420\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The 'out' argument to jnp.trace is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_check_arraylike\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m                     if not _arraylike(arg))\n\u001b[1;32m    569\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{} requires ndarray or scalar arguments, got {} at position {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_no_float0s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: trace requires ndarray or scalar arguments, got <class 'neural_tangents._src.utils.kernel.Kernel'> at position 0."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_positive_list = list(map(list, data.data['train_positive_set']))"
      ],
      "metadata": {
        "id": "-OHtDPDet5Ri"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " eval_context = data.data['train_matrix']"
      ],
      "metadata": {
        "id": "knXv96jzueRJ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_predict = data.data['val_positive_set']"
      ],
      "metadata": {
        "id": "7rJkjw87upiD"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset(hyper_params)\n",
        "hyper_params = copy.deepcopy(data.hyper_params) # Updated w/ data-stats"
      ],
      "metadata": {
        "id": "kWIAPWhZx6p_",
        "outputId": "e5b35cce-86c7-4346-f8a1-eb840782632b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6040 3706\n",
            "# users: 6040\n",
            "# items: 3706\n",
            "# interactions: 791718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  kernelized_rr_forward, kernel_fn = make_kernelized_rr_forward(hyper_params)\n",
        "  sampled_matrix = data.sample_users(hyper_params['user_support'])"
      ],
      "metadata": {
        "id": "nGw3jmogxeXl"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bsz = 20_000 # These many users\n",
        "for i in range(0, 6040, bsz):\n",
        "    temp_preds = kernelized_rr_forward(sampled_matrix, eval_context[i:i+bsz].todense(), reg = hyper_params['lamda'])\n",
        "    #np.save('/content/drive/MyDrive/infinite_ac_cf_main/kernel', temp_preds) # dual parameter save\n",
        "    metrics, temp_preds, temp_y = evaluate_batch(\n",
        "        data.data['negatives'][i:i+bsz], np.array(temp_preds), \n",
        "        train_positive_list[i:i+bsz], to_predict[i:i+bsz], item_propensity, \n",
        "        topk, metrics)"
      ],
      "metadata": {
        "id": "Y5_kV6CZu3Mp",
        "outputId": "f4ef4c63-8947-427e-b316-8a04682d4edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-98-d2e742bb1e46>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  data.data['negatives'][i:i+bsz], np.array(temp_preds),\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-d2e742bb1e46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#np.save('/content/drive/MyDrive/infinite_ac_cf_main/kernel', temp_preds) # dual parameter save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     metrics, temp_preds, temp_y = evaluate_batch(\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'negatives'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_positive_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_propensity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         topk, metrics)\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (6040,3706) into shape (6040,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(temp_preds)"
      ],
      "metadata": {
        "id": "r4xKZQdwyP4g",
        "outputId": "02447c03-8568-47da-d767-2af73b931a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-107-561551f354f6>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  np.array(temp_preds)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-561551f354f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (6040,3706) into shape (6040,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(hyper_params, kernelized_rr_forward, data, item_propensity, train_x, topk = [ 10, 100 ], test_set_eval = False):\n",
        "    preds, y_binary, metrics = [], [], {}\n",
        "    for kind in [ 'HR', 'NDCG', 'PSP' ]:\n",
        "        for k in topk: \n",
        "            metrics['{}@{}'.format(kind, k)] = 0.0\n",
        "\n",
        "    # Train positive set -- these items will be set to -infinity while prediction on the val/test set\n",
        "    train_positive_list = list(map(list, data.data['train_positive_set']))\n",
        "    if test_set_eval:\n",
        "        for u in range(len(train_positive_list)): train_positive_list[u] += list(data.data['val_positive_set'][u])\n",
        "\n",
        "    # Train positive interactions (in matrix form) as context for prediction on val/test set\n",
        "    eval_context = data.data['train_matrix']\n",
        "    if test_set_eval: eval_context += data.data['val_matrix']\n",
        "\n",
        "    # What needs to be predicted\n",
        "    to_predict = data.data['val_positive_set']\n",
        "    if test_set_eval: to_predict = data.data['test_positive_set']\n",
        "\n",
        "    bsz = 20_000 # These many users\n",
        "    for i in range(0, hyper_params['num_users'], bsz):\n",
        "        temp_preds = kernelized_rr_forward(train_x, eval_context[i:i+bsz].todense(), reg = hyper_params['lamda'])\n",
        "        #np.save('/content/drive/MyDrive/infinite_ac_cf_main/kernel', temp_preds) # dual parameter save\n",
        "        metrics, temp_preds, temp_y = evaluate_batch(\n",
        "            data.data['negatives'][i:i+bsz], np.array(temp_preds), \n",
        "            train_positive_list[i:i+bsz], to_predict[i:i+bsz], item_propensity, \n",
        "            topk, metrics\n",
        "        )"
      ],
      "metadata": {
        "id": "Mu7MUqodsjp4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}